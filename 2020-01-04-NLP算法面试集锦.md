

<u>**2020-01-04**</u>

**1.文本表示模型及其优缺点**

可分为传统模型和词嵌入模型。

传统模型包括词袋模型、TF-IDF、n-gram、LDA，其中词袋模型主要是依据字典来计算每个样本中的词语对应的频率/有无出现特征；而TF-IDF主要是用来衡量某个单词对于语义区别的重要性；n-gram主要是通过滑动窗口的形式将连续的单词作为对应特征；LDA是通过分解“文档-单词”矩阵来得到“文档-主题”和“主题-单词”两个概率分布，主要目的是计算每篇文档的主题分布。这一类传统模型实现较为简单，效率高，但所获得的的特征不包含语义信息，且易造成维度灾难。

词嵌入模型属于深度学习的范畴，旨在将每个单词映射成一个低维的稠密向量，包括word2vec、Glove、fasttext等算法，其中最常用的是word2vec，其次是fasttext。在语义获取上更有优势。

**2.word2vec是如何工作的？它和LDA有什么区别和联系呢？**

关于word2vec的工作原理可参考[大话NLP领域的传统词向量预训练]([https://vincent131499.github.io/2020/01/03/%E5%A4%A7%E8%AF%9DNLP%E9%A2%86%E5%9F%9F%E7%9A%84%E4%BC%A0%E7%BB%9F%E8%AF%8D%E5%90%91%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83/](https://vincent131499.github.io/2020/01/03/大话NLP领域的传统词向量预训练/))。

而针对word2vec与LDA的区别与联系：

LDA是通过分解“文档-单词”矩阵来得到“文档-主题”和“主题-单词”两个概率分布，主要目的是计算每篇文档的主题分布；而word2vec是通过浅层的三层网络来进行参数矩阵的训练，最终得到每个单词对应的词向量。

在计算一个语料库中各个单词之间的相似度可以使用word2vec来计算，而要计算一个语料库中各个文档之间的相似度，那还是LDA比较合适，通过文章主题会得到更好的相似度衡量。

**3.处理文本数据时，RNN比CNN有什么特点？**

传统文本处理以TF-IDF和词袋模型作为特征，会使得模型丢失重要的语序特征。为了捕捉这个语序特征，提出了使用CNN和RNN来处理文本信息。

CNN一般会接受定长的向量作为输入，然后通过滑动窗口加池化的方法来捕捉重要的局部特征，但对于单词间的长距离依赖特征难以学习。而RNN对于处理文本这样的序列信息有着天然的优势，因为它是将上一时刻的输出特征作为下一时刻的输入来进行传递处理，对于单词间的长距离依赖特征能够很好地捕捉。

**4.RNN为什么会出现梯度消失或梯度爆炸？有哪些解决方案？**

定义：度神经网络训练的时候，采用的是反向传播方式，该方式使用链式求导，计算每层梯度的时候会涉及一些连乘 作，因此如果网络过深。那么如果连乘的因子大部分小于1，最后乘积的结果可能趋于0，也就是梯度消失，后面的网络层的参数不发生变化.那么如果连乘的因子大部分大于1，最后乘积可能趋于无穷，这就是梯度爆炸。

梯度消失或者梯度爆炸是由RNN中的sigmoid函数引起的，当网络层数较多时，随着预测误差沿神经网络每一层反向传播，当最初初始化的网络权值w小于1时，前面的层比后面的层梯度变化更小，梯度呈指数减小，故变化更慢，从而引起梯度消失；反之当w比较大时，前面的网络层比后面的网络层梯度变化更快，梯度呈指数增长，则会引起梯度爆炸。

解决方案：（1）ReLU替代sigmoid函数；（2）BN（Batch Normalization）;（3）LSTM的结构设计也可以改善RNN中的梯度消失问题；（4）Resnet（残差网络）

Note：在RNN采用Relu作为激活函数时，应将权重矩阵w初始化为单位矩阵。

**5.LSTM是如何实现长短期记忆功能的？**

[LSTM模型细节结构讲解](https://blog.csdn.net/dream_catcher_10/article/details/48522339)

LSTM单元主要是由输入门、遗忘门和输出门三个结构组成；其中输入门用于控制当前时刻有哪些重要信息可以输入到记忆单元；遗忘门用于控制前一步的记忆单元有哪些无用信息会被遗忘掉；而输出门则是决定当前的记忆单元有哪些重要信息可以输出。

在一个训练好的网络中，当输入序列没有重要信息时，LSTM遗忘门的值接近于1，输入门接近于0，此时过去的记忆会被保存，从而实现了长期记忆；当输入的序列中存在重要信息时，LSTM会将其存在记忆中，此时输入门的值接近于1；当输入的序列中存在重要信息且该信息意味着之前的记忆不再重要的时候，输入门接近于1,，遗忘门接近于0，这样旧的记忆被遗忘，新的记忆被保存。经过这样的设计，整个网络更容易学习到序列之间的长期依赖。

6.LSTM中各模块分别使用什么激活函数，可以使用别的激活函数吗？

LSTM中的输入门、遗忘门和输出门的激活函数都是sigmoid，在生成候选记忆时，使用双曲正切函数Tanh作为激活函数。

why-细节：

这两个函数都是饱和的，在输入达到一定值的情况下，输出不会发生明显变化。如果是非饱和的激活函数，比如Relu，那么就难以实现门控的效果。

Sigmoid函数的输出在0~1之间，符合门控的物理意义。且当输入较小或较大时，其输出会非常接近0或1，从而保证该门开或关。

在生成候选记忆时，使用Tanh函数，因为其输出在-1~1之间，这与大多数场景下特征分布是0中心的吻合。此外，Tanh在输入为0的附近比Sigmoid有更大梯度，通常使模型收敛更快。

可以使用别的激活函数吗？---激活函数的选取并不是一成不变的，在原始LSTM中，使用的是sigmoid函数的变种；但是实际上在门控机制中，大多都使用了sigmoid。





