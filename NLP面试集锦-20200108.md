**<u>2020-01-08</u>**

**16.决策树的概念？有哪些常用的启发函数？如何对决策树进行剪枝？**

概念：决策树是一种自上而下，对样本数据进行树形分类的过程。节点分为内部节点和叶子节点。每个内部节点代表一个特征，叶节点代表类别。从顶部节点开始，所有样本聚在一起，经过根节点和不同内部子节点的划分，所有样本都被归到某一个类别（叶节点）。

启发式函数：ID3、C4.5、CART

ID3：最大信息增益，选择能够带来最大的信息增益的那个特征；

C4.5：最大信息增益比。

CART：最大基尼系数。基尼指数反映了从数据集中随机抽取两个样本，其类别不一致的概率。CART每次选取使基尼系数最小的特征。

剪枝方法：决策树容易过拟合，需要剪枝。分为预剪枝和后剪枝。

预剪枝：在对树中节点扩展之前，先计算当前划分能否带来模型泛化能力的提升，若不能，则不再继续生长子树。

后剪枝：先生成一个完整的决策树，然后自底向上计算是否剪枝（将子树切除，用叶节点代替）。通过能够提升验证集准确率来计算。

**17.介绍一下GBDT**

梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是Boosting算法中非常流行的一个。

Gradient Boosting的流程：在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练出一个新的弱分类器进行拟合并计算出该弱分类器的权重。最终实现对模型的更新。

而采用决策树作为弱分类器的Gradient Boosting算法成为GBDT。GBDT中使用的决策树通常为CART。

**18.Xgboost与GBDT的区别和联系？**

（1）GBDT是机器学习算法，Xgboost是该算法的工程实现；

（2）在使用CART作为基分类器的时候，XGBoost显式地加入了**正则项**来控制模型复杂度，有利于防止过拟合；

（3）数据采样：传统的GBDT在每轮迭代时使用全部数据，XGBoost支持对数据进行采样；

（4）缺失值处理：传统的GBDT没有设计对缺失值的处理，XGBoost能自动学出缺失值处理策略

**19.深度学习中常见的优化器？**

（1）SGD

目前SGD一般都是指mini-batch gradient descent，这是神经网络中大多使用的优化方法。

SGD就是每一次迭代计算mini-batch的梯度，然后对参数更新。

这里说一下局部最优点和鞍点。

两者相似：在这个点的导数为0；

区别：是否在各个维度都是最低点。只要某个一阶导数为0的点在某个维度上是最高点而不是最低点，那它就是鞍点。

（2）Momentum/动量更新

积累之前的动量来替代真正的梯度。通过在相关方向加速SGD，抑制震荡，加快收敛。

（3）Adagrad

对学习率进行约束，通过以往的梯度自适应更新学习率，不同的参数具有不同的学习率。十分适合处理稀疏数据（对常出现的特征进行小幅度更新，不常出现的特征进行大幅度更新）。

（4）RMSprop

可以算作Adadelta的一种特例，适合处理非平稳目标，结合RNN使用效果最好。

（5）Adam

本质上是带有动量的RMSprop，利用梯度的一阶矩阵和二阶矩阵估计动态调整每个参数的学习率。结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点。

**20.如何防止过拟合以及为什么可以？**

（1）数据增强

数据集扩增

（2）early stopping

一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。

（3）正则化限制

这种方法一般是在目标函数或代价函数后面加上一个正则项，一般有L1正则、L2正则等。

L1正则为什么可以防止过拟合？

添加L1正则项可以让那些原先处于0附近的参数往0移动，使得部分参数为0，进而降低模型的复杂度（模型的复杂度由参数决定），从而防止过拟合，提高模型的泛化能力。

L2正则为什么可以防止过拟合？

L2正则项可以加速参数变小，而更小的参数意味着模型的复杂度更低。

L1和L2正则的区别？

区别1：L1是在loss函数后加上参数的1范数（|w|），即对应参数向量的绝对值之和，而L2是在loss函数后加上参数的2范数（sigma(w^2)），即对应参数向量的平方和的平方根;

区别2：L1会产生稀疏的特征，L2会产生更多的接近0的特征。L1在特征选择时候非常有用，L2只是一种规则化，提升模型的泛化能力。

区别3：L1正则先验服从拉普拉斯分布，L2正则先验服从高斯分布。

L1在0出不可导，怎么处理？

可以使用Proximal Algorithms或者ADMM来解决。

（4）Dropout

通过随机删除隐藏层的神经元个数来防止过拟合。

为什么可以防止过拟合？

原因1：不同的网络训练时可能产生不能的过拟合，而dropout对丢失隐藏层神经单元的多个不同网络**取平均**，其中互为“反向”的拟合相互抵消就可以达到整体上过拟合减少；

原因2：减少神经元之间复杂的**共适应**关系。dropout导致两个神经元不一定每次都在同一个网络中出现，这样就可以使网络去学习更加鲁棒的特征，提升模型的泛化能力。

（5）Batch Normalization

BN实际使用时会将特征强制性的归到**均值为0**，**方差为1**的数学模型下。有两个功能：一个是加快训练和收敛，一个是防止过拟合。

为什么可以防止过拟合？

BN的使用使得同一个样本的输出不仅仅取决于这个样本本身的特征，也取决于与这个样本属于同一个mini-batch的其他样本的特征，进而增强模型的泛化能力。

如何加快训练和收敛速度？

BN将每层网络的数据都转换到均值为0，方差为1的状态下，一方面，数据的分布相同，训练比较容易收敛，另一方面，均值为0，方差为1的状态下，梯度计算时会产生较大的梯度值，可以加快参数的训练。更进一步，BN也可以很好的控制梯度消失和梯度爆炸现象。

BN最大的优点就是允许网络使用较大的学习率进行训练，加快网络的训练速度。



