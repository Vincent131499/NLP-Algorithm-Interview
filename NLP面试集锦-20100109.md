<u>**2020-01-09**</u>

**21.LR和SVM的区别与联系？**

联系：

（1）LR和SVM都可以用于处理分类问题，且一般都用于处理线性二分类问题（改进的情况下可以用于处理多分类）；

（2）两个方法都可以增加正则项，如L1、L2等。所以在很多实验中，两种算法的结果是很接近的。

区别：

1.LR是参数模型，SVM是非参数模型；

2.目标函数：逻辑回归采用的是Logistical Loss，SVM采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减小与分类关系较小的数据点的权重。

3.SVM的处理方法是只考虑Support Vectors，也就是和分类最相关的少数点，去学习分类器；而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。

**22.LR和线性回归的区别与联系？**

二者都是广义的线性回归。

1.分类与回归：回归模型就是预测一个连续变量（如价格、降水量）。在分类问题中则是预测属于某类的概率。

2.输出：直接使用线性回归的输出作为概率存在问题，因为其值有可能小于0或者大于1，而逻辑回归的输出在[0,1]之间。

3.优化目标函数：线性回归模型的优化目标函数是最小二乘，而逻辑回归则是似然函数。

**23.SVM模型的推导**



**24.LR的原理和loss的推导**

前面的线性回归，我们已经得到y=wx+b。它是实数，y的取值范围可以是（负无穷，正无穷）。现在，我们不想让它的值这么大，所以我们就想把这个值给压缩一下，压缩到[0,1]。什么函数可以干这个事呢？研究人员发现signomid函数就有这个功能。

**25.常见损失函数**

（1）线性回归损失函数

<a href="https://www.codecogs.com/eqnedit.php?latex=C=\sum_{i=1}^{n}(y_{i}-y_{i}^{-})^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C=\sum_{i=1}^{n}(y_{i}-y_{i}^{-})^{2}" title="C=\sum_{i=1}^{n}(y_{i}-y_{i}^{-})^{2}" /></a>

这里的损失函数为什么要用平方差形式？

使用平方形式的时候，使用的是“最小二乘”的思想，这里的“二乘”指的是用平方来度量观测点与估计点的距离（远近），“最小”指的是参数值要保证各个观测点与估计点的距离的平方和达到最小。

(2)SVM损失函数

<a href="https://www.codecogs.com/eqnedit.php?latex=L_{i}=\sum_{j\neq&space;y_{i}}max(0,S_{j}-S_{y_{i}}&plus;\Delta&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L_{i}=\sum_{j\neq&space;y_{i}}max(0,S_{j}-S_{y_{i}}&plus;\Delta&space;)" title="L_{i}=\sum_{j\neq y_{i}}max(0,S_{j}-S_{y_{i}}+\Delta )" /></a>

其中，S_{yi}表示真实类别得分，S_{j}表示其他类别的得分。delta表示为边界值，L_{i}表示第i个输出的损失值。

（3）交叉熵损失函数

<a href="https://www.codecogs.com/eqnedit.php?latex=L=-[ylog\hat{y}&plus;(1-y)log(1-\hat{y})]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L=-[ylog\hat{y}&plus;(1-y)log(1-\hat{y})]" title="L=-[ylog\hat{y}+(1-y)log(1-\hat{y})]" /></a>



**26.GRU**

GRU即Gated Recurrent Unit。之前说到为了克服RNN的梯度消失和无法很好地处理远距离依赖提出了LSTM，而GRU是LSTM的一个变体，它保持了LSTM效果的同时又使结构更加简单。

GRU模型只有两个门：更新门和重置门，更新门用于控制前一时刻的状态信息有多少比例可以输入到下一状态中，更新门的权值越大说明前一时刻的状态信息带入越多；而重置门用于控制忽略前一时刻的状态信息的程度，重置门的值越小说明忽略的越多。

GRU将LSTM中的输入门和遗忘门合并成一个单一的更新门。

GRU与LSTM的区别：

（1）memory的控制：LSTM用输出门控制，然后传递给下一单元；而GRU直接将其传递给下一个单元，不做任何控制。

（2）上一时刻的信息：LSTM计算新的记忆时不对上一时刻的信息做任何控制，通过使用遗忘门独立实现；而GRU计算新的记忆时利用重置门对上一时刻的信息进行控制。

**27.Memory Network**

如RNN等网络的记忆存储太小，不能精确地记住过去的事实。由此提出了Memory Network。

一个Memory Network由一个记忆数组m和四个组件（输入I，泛化G、输出O、回答R）组成。

四个组件的作用：

I（输入特征映射）： 将输入转换为记忆网络内部特征的表示。给定输入x，可以是字符、单词、句子等不同的粒度，通过I(x)得到记忆网络内部的特征。

G :(更新记忆） - 使用新的输入更新记忆数组m。

O：（输出） - 在记忆数组m更新完以后，就可以将输入和记忆单元联系起来，根据输入选择与之相关的记忆单元。

R：（输出回答） - 得到了输入编码向量I(x)，记忆数组m和需要的支持事实，就可以根据问题来得到需要的答案了。文中给出了一个简单的R()函数，将输入和选择的记忆单元与此表中的每个单词进行评分Sr,然后选择得分最大的单词作为回答。



